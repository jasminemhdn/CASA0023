[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023: Remotely Sensing Cities and Environment",
    "section": "",
    "text": "Personal Introduction\nMy name is Jasmine Mahdani and I am from Indonesia. I graduated from ITB (Bandung Institute of Technology) in 2022, majoring in Geodesy and Geomatic Engineering. During my undergraduate years, I had been actively engaged in research particularly for urban related issues using GIS and remote sensing. My research topic focus on water and sanitation access, slum identification, and disaster risk assessment. My enthusiasm to urban issues and GIS continues after I graduate that leads me to choose urban spatial researcher as my career path. However, doing spatial analysis for urban issues is quite tricky because the coverage area is relatively small and finding open source detailed data set in Indonesia is extremely hard. Moreover, I also have limited knowledge on how urban system works. Therefore, I decided to pursue master degree in Urban Spatial Science at UCL (University College London) to deepen my knowledge in urban analytics and to support my career path as a researcher. In Urban Spatial Science, I take the Urban Modelling and Simulation pathway and Remotely Sensing Cities and Environment modules in hope I can answer the challenge of using open sourced medium spatial resolution remote sensing data for urban research that mostly have small coverage area.",
    "crumbs": [
      "Personal Introduction"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Introduction to Remote Sensing",
    "section": "",
    "text": "1.1 Summary: Remote Sensing Definition and How it Works\nRemote sensing is the practice of obtaining information about the Earth’s surface, using images acquired from airborne or spaceborne vehicles by measuring reflected, emitted, or returned electromagnetic radiation (Ruth DeFries, 2013). The objective of this technology is to provide observation of physical parameter in a mapping frame at a given time period (Toth & Jozkow, 2016).\nRemote sensing captures the electromagnetic (EM) radiation that is reflected from earth’s surface. EM wave is an energy that travels through the light. All matters with absolute temperature above zero reflect and emit EM waves of various length (J. M. Read and M. Torrado, 2009). A material that fully capable of absorbing and re-emitting all EM energy that it receives is called a blackbody, but it is rare and most natural objects only absorb some of the energy (Tempfli K. et al., 2009). The difference in how objects absorb and reflect energy is what’s make every earth’s feature has its own signature characteristic which is recorded as digital number (DN) in each pixel of images using the sensors.\nIn remote sensing, there are two type of sensors, passive and active that are used for different applications. Most sensors use passive systems which can only capture the reflection of sun’s energy and work during daylight. Therefore, this sensors’ orbit sometimes are set to follow’s the sun (sun-synchronous). The electromagnetic spectrum recorded by the sensors range from ultraviolet (UV), visible (red, green, blue), infrared (IR), and microwave. Meanwhile, active remote sensing can radiate its own energy and measure the amount of radiation returned to the sensor. Active sensors can penetrate through clouds and won’t be affected by daylight or weather conditions. Example of active sensors included SAR and LIDAR.\nAlthough remote sensing is great for collecting data in inaccessible area and cheaper for mapping large areas, there are 4 type of resolution that needs to be considered when using it: spatial, spectral, temporal, and radiometric. Spatial resolution refers to the size of area measured which is represented by each pixel. Spectral refer to the range of wavelength the sensor is sensitive to. Radiometric refer to the difference in radiation intensity. Temporal characteristics refer to the time of image acquisition. These resolutions need to be considered based on the purpose of the mapping.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "week1.html#application-example-of-landsat-and-sentinel-for-urban-heat-island-uhi",
    "href": "week1.html#application-example-of-landsat-and-sentinel-for-urban-heat-island-uhi",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Application: Example of Landsat and Sentinel for Urban Heat Island (UHI)",
    "text": "1.2 Application: Example of Landsat and Sentinel for Urban Heat Island (UHI)\nWith the growing technology, remote sensing becomes more advanced and freely available. The development of single sensor to multiple sensors allows spatial researcher to do environmental and social research using it. The example of application includes biodiversity monitoring, crop classification, hazards modelling, Urban Heat Island monitoring, LULC classification, and carbon sequestration modelling (Roy et al., 2017). Sometimes people compare and combine multiple sensors from different platform to get accurate and comprehensive results because different sensors capturing the same area might get different reflectance values. Examples of known satellites for research are Terra, MODIS, SPOT, Landsat, and Sentinel (Toth & Jozkow, 2016).\nThe paper by Rech et al., (2024) gives example of using Landsat 8 TIRS band for mapping Surface Urban Heat Islands (SUHI) and assess the SUHI relation to albedo, elevation, land surface emissivity, and vegetation cover across Local Climate Zones (LCZ) in Florianopolis, Brazil. The paper assessing the variability of SUHI between day and night, different LCZ and different surface using multiple statistical approaches. Although the paper mentioned using Practical Single-Channel (PSC) algorithm to derive LST, the result does not explain the accuracy of the methods or the LST before used for analysis and instead only focuses on the variability with the parameters. Since the study area is a humid region, the use of PSC method give concern on its reliability. However, I think the author has tried it best to find images with low cloud cover and no rain. Unfortunately, there’s only one pair that meets these criteria, so it’s not feasible to conduct the study for different seasons. Moreover, since the TIRS band already resampled to 30 meters, I think it would be good if they zoom in to some specific areas to show variability.\nMeanwhile, the paper by García and Díaz (2021) used Sentinel-3 to Meanwhile, the paper by García and Díaz (2021) used Sentinel-3 to derive LST and applied Split-Window Correction for assessing Urban Heat Island and factors contributing to it in Granada, Spain. Different from previous one, this study compares the value derived from Sentinel-3 with in-situ analysis. The result shows Sentinel-3 temperature is a little bit higher. The study also conduct analysis for different seasons because Sentinel-3 temporal resolution is daily giving more available data to used. However, the Sentinel-3 1 km resolution might not be as detailed as Landsat 8, even though the author had tried to resample it to 100 meters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "week1.html#reflection-challenges-in-remote-sensing-and-potential-future-study",
    "href": "week1.html#reflection-challenges-in-remote-sensing-and-potential-future-study",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflection: Challenges in Remote Sensing and Potential Future Study",
    "text": "1.3 Reflection: Challenges in Remote Sensing and Potential Future Study\nSince I have been learning about remote sensing on my undergraduate degree before, the material about sensors and electromagnetic reflectance are already familiar for me. I also have used some remote sensing data for my research and since the first time I know about it, I always think remote sensing mapping is a cheaper method for large area compared to other mapping techniques. However, when I did the practical using SNAP for Sentinel-2 and Landsat-8 data, I started to think even though the medium spatial resolution remote sensing images are freely available, we still need high specification computer to process it because one tiles comprised of several bands are huge and my laptop somewhat need to work hard to do the resampling and other practical. It also made me realise that it’s not quite efficient to do preprocessing of these images from scratch when you only want to do research on very small area especially when the boundary does not overlap perfectly with the image. I think using Google Earth Engine (GEE) which would be explained in the later weeks is better because they provide some already pre-processed images and its based-on cloud, so you don’t need high specification computer. Beside this issue, I also think that this ‘cheaper method’ only applies to regional-based analysis that can use medium spatial resolution like Landsat and Sentinel-2. Meanwhile, if you want to do a detailed analysis on city like how the UHI effect on different surface and LCZ, especially for building level, it will need high spatial resolution which are mostly still commercial. The resampling methods may give us immediate solution for this although we still need to consider the impact of each method that may affect the analysis. However, despite all the challenges in remote sensing approaches, I still think it’s the most robust method in the present that can give comprehensive result when its used and combined with other dataset appropriately. To this day, I still have a goal to do UHI analysis on building level using open-source remote sensing data. I believe remote sensing technology would develop even further that gives more better resolution from the existing ones.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Remote Sensing</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Xaringan Presentation and Quarto Book",
    "section": "",
    "text": "2.1 Reflection: Make Slides using Xaringan\nThis is my first time learning how to make presentation by writing it in Xaringan instead of directly place the words and figures in slides like in Microsoft Powerpoint. The Xaringan might quite simple, but I actually prefer a simple presentation. It is good that I don’t have to think about symmetrical placement of the word and figures because they are already placed automatically and I just have to specify the location using middle, center, or .pull-left and .pull-right. Also, the use of github making it easier to collaborate with colleagues and can be accessible for everyone at anytime and anywhere. My favorite part is the tab part in single slides. Usually for information like resolutions for different instrument (slides 4), I would display it as a table, but from now on I think I will use tab a lot since it gives more space for explanation but because they are placed in one slides it makes more efficient.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Xaringan Presentation and Quarto Book</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "",
    "text": "3.1 Summary: Type of Corrections and Enhancement\nRemote sensing images contain bias and error from earth’s rotation, terrain and atmospheric condition that may affect the surface reflectance. Below is the summary of correction applied to remote sensing for the pre-processing process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week3.html#summary-type-of-corrections-and-enhancement",
    "href": "week3.html#summary-type-of-corrections-and-enhancement",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "",
    "text": "Geometric Correction\nGeometric correction is used to remove or reduce the effect of Earth rotation and terrain during the scanning of an image. The table below shows the example of geometric correction approaches.\n\nGeometric Correction Methods\n\n\n\n\n\n\nMethods\nDescription\n\n\n\n\nGeoreferencing\nThe process of link an image to a map projection by using geometric transformation. Points in identical places that captured in two different images or systems is the key points in georeferencing.\n\n\nGeocoding\nThe process use to transform row/column structure of the image (resampling). Geocoding is required to combine different images.\n\n\n\n\n\nRadiometric Correction\nThe radiometric correction deals with the radiance values of reflected polychromatic solar radiation and the emitted thermal radiance from Earth’s surface. The atmosphere cause the radiance value to be altered when recorded by the satellite. In general the radiometric correction is divided into cosmetic correction and atmospheric correction.\n\nRadiometric Correction Methods\n\n\n\n\n\n\nMethods\nDescription\n\n\n\n\nCosmetic\nCosmetic correction use filters, image stretching, and image enhancement to correct visible errors and noise in the image data without involving atmospheric model.\n\n\nAtmospheric\nAtmospheric correction is applied to remove the atmospheric effect by using models. There are two types of this correction: relative (does not require atmospheric component) and absolute (require atmospheric component).\n\n\n\n\n\nEnchancement\nImage enhancement is the procedure of improving the quality and information content of original data (Haldar, 2018). The table below explain each of the methods.\n\nImage Enhancement Methods\n\n\n\n\n\n\nMethods\nDescription\n\n\n\n\nContrast Enhancemenet\nContrast enhancement or stretching is performed by linear transformation expanding the original range of gray level.\n\n\nSpatial Filtering\nSpatial filtering improves the naturally occurring linear features like fault, shear zones, and lineaments.\n\n\nDensity Slicing\nDensity slicing converts the continuous gray tone range into a series of density intervals marked by a separate color or symbol to represent different features.\n\n\nFCC\nFCC is commonly used in remote sensing compared to true colors because of the absence of a pure blue color band because further scattering is dominant in the blue wavelength.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "3.2 Application:",
    "text": "3.2 Application:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "3.3 Reflection:",
    "text": "3.3 Reflection:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "",
    "text": "5.1 Summary: GEE Function and Interface\nGoogle Earth Engine (GEE) is a cloud-based remote sensing data processing that allows us to access huge variety of open-source remote sensing product such as Landsat, Sentinel, and MODIS or raster product made by other people such as Impervious Surface. To use GEE, we must create an account first using Google account. GEE is connected directly to Google Drive, so we can save the unprocessed or already processed data into Google Drive and download it in case we want to use it for processing or lay outing the maps in GIS application such as QGIS. However, if the data too big, it would require large Google Drive space. Another alternative to store the processed data or present the maps online is by using Asset and Web Apps. This allows us to access the data we have already processed before for another analysis.\nJavaScript is the main programming language used in GEE. But, we could also use Python when GEE is connected to Google Colab, Jupyter Notebook or QGIS. Even though it seems that JavaScript is harder to understand, GEE provides explanation of every syntax in Docs Panel. The picture below shows the GEE interface.\nAnd this is the explanation of each part of the interface:\nThe picture below shows the example of MODIS Terra data catalog in GEE. In this catalog, there are information containing data availability, description of the data, raster bands and its spatial resolution, terms of use, citation, and dois. If we want to use this data, we can click IMPORT button in bottom right.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week5.html#summary-gee-function-and-interface",
    "href": "week5.html#summary-gee-function-and-interface",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "",
    "text": "Script Panel: this panel store the scripts that has been saved and it allows us to create and load scripts.\nDocs Panel: store the documentation and function description of GEE commands.\nAssets Panel: a storage space to upload raster or vector data in GEE.\nConsole Panel: displays outputs, print statements, and errors when running scripts.\nMap Panel: display geospatial datasets and analysis results.\nTask Manager: manages long-running tasks like exporting images, tables, or maps.\nInspector Panel: allows us to click on the map and inspect pixel values.\nGet Link: generate a shareable URL for the script and map view.\nSave: save the current script.\nRun: executes the script and applies it to the Map Panel.\nReset: clears all variables, print outputs, and layers.\nApps: provides access to custom web apps created in GEE.\nSearch Bar: find datasets, function, or scripts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week5.html#application-identification-of-plastic-waste-accumulation-in-gee",
    "href": "week5.html#application-identification-of-plastic-waste-accumulation-in-gee",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "5.2 Application: Identification of Plastic Waste Accumulation in GEE",
    "text": "5.2 Application: Identification of Plastic Waste Accumulation in GEE",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "5.3 Reflection:",
    "text": "5.3 Reflection:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "",
    "text": "6.1 Summary: Difference Between Supervised and Unsupervised Classification\nClassification is the most common methods used in remote sensing. Example of basic classification is land cover classification where the pixel is classified into each category such as bare lands, built up areas, forests, agricultures, and water bodies. In general, image classification is divided into two types: Supervised and Unsupervised. Below is the explanation of each classification methods:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week6.html#summary-difference-between-supervised-and-unsupervised-classification",
    "href": "week6.html#summary-difference-between-supervised-and-unsupervised-classification",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "",
    "text": "Supervised Classification\nSupervised classification makes classes based on pattern of previous information. Machine learning algorithm is categorised as supervised classification. Manual classification requires us to define each class and create area of interest (AOI) as examples for each classes, while machine learning methods use training data to make classification. Examples of supervised classification methods are Maximum Likelihood, Support Vector Machine (SVM), and Random Forest (RF).\n\nMaximum Likelihood: probabilistic method that assumes each class has normal distribution.\nSupport Vector Machine (SVM): machine learning classifier that define the best boundary (hyperplane) to separate each class.\nRandom Forest (RF): machine learning algorithm that builds multiple decision trees.\n\n\n\nUnsupervised Classification\nUnsupervised classification is a method that make classification based on the same spectral properties. We don’t have to define each class and know a priori information, but we have to determine the number of class or cluster that we want to create through the classification process. Example of unsupervised clustering is K-Means and ISODATA.\n\nK-Means: assign pixel to k clusters based on similarity of pixel value or digital number.\nISODATA: adjust the number of clusters dynamically.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week6.html#application-machine-learning-techniques-in-animal-habitat-suitability",
    "href": "week6.html#application-machine-learning-techniques-in-animal-habitat-suitability",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "6.2 Application: Machine Learning Techniques in Animal Habitat Suitability",
    "text": "6.2 Application: Machine Learning Techniques in Animal Habitat Suitability",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "6.3 Reflection:",
    "text": "6.3 Reflection:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "",
    "text": "7.1 Summary: More Type of Classification and Accuracy Assessment\nBeside categorised based on the use of training data and a priori knowledge, remote sensing classification can be divided into sub-pixel-based and object-based classification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "",
    "text": "Sub-pixel-based Classification\n\n\nObject-based Classification\n\n\nClassification Accuracy Assessment",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  },
  {
    "objectID": "week7.html#application-building-footprints-extraction",
    "href": "week7.html#application-building-footprints-extraction",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "7.2 Application: Building Footprints Extraction",
    "text": "7.2 Application: Building Footprints Extraction\nBoth papers by Chen et al. (2020) and Li et al. (2019) focus on building extraction using high-resolution satellite imagery but apply different approaches. Chen et al. (2020) uses object-based classification by combining SLIC segmentation and a multi-modal CNN that processes both PAN and MS images from GF-2 in Wuhan, China. Meanwhile, Li et al. (2019) applies a pixel-based semantic segmentation using U-Net on pan-sharpened WorldView-3 imagery across four global cities. I find Chen et al.’s method interesting because it extracts building shapes more accurately by combining spatial and spectral features through object-based segments. The use of radiometric and atmospheric correction (FLAASH) before feature extraction is a strength, as it helps reduce spectral inconsistencies. However, the method relies heavily on segmentation quality, and its performance may drop if the objects are poorly defined. Li et al. (2019), on the other hand, takes a more scalable approach. Testing their method across diverse cities makes it more applicable globally. I like that they combine multi-source GIS data for validation and apply data augmentation and post-processing to improve results. However, there is no clear mention of atmospheric or radiometric correction, which could affect consistency, especially when using data from different cities. Their result in Khartoum was also less accurate, showing challenges in detecting buildings in low-contrast or shaded areas.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "7.3 Reflection:",
    "text": "7.3 Reflection:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "8  Introduction to Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "8.1 Summary: How SAR Works\nSynthetic Aperture Radar is an active remote sensing sensor that can generate it’s own energy, can penetrate through the clouds and operates day and night. SAR is using microwave frequencies to capture image of Earth’s surface. SAR works by emitting radar pulses toward Earth’s surface, then the pulses interact with surfaces, scattering signals differently based on material properties, the sensors then detect and process returned signals to create images based on intensity, phase, and polarization. The table below shows the common SAR frequency bands\nThe video below by NASA ARSET provides comprehensive explanation about SAR:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "week8.html#summary-how-sar-works",
    "href": "week8.html#summary-how-sar-works",
    "title": "8  Introduction to Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "SAR Bands\n\n\n\n\n\n\n\n\n\nBand\nFrequency\nWavelength\nSatellite Examples\nApplication\n\n\n\n\nX-band\n8–12 GHz\n~3 cm\nTerraSAR-X, Cosmo-SkyMed\nUrban areas, high spatial detail\n\n\nC-band\n4–8 GHz\n~5.6 cm\nSentinel-1, RADARSAT\nAgriculture, flood mapping, rapid change\n\n\nL-band\n1–2 GHz\n~23 cm\nALOS PALSAR, SAOCOM\nVegetation, forests, subsurface features",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "week8.html#application-insar-for-disaster-mapping",
    "href": "week8.html#application-insar-for-disaster-mapping",
    "title": "8  Introduction to Synthetic Aperture Radar (SAR)",
    "section": "8.2 Application: InSAR for Disaster Mapping",
    "text": "8.2 Application: InSAR for Disaster Mapping\nOne of the most common application of SAR is for disaster or post disaster mapping especially for natural hazard that relate to deformation or earth movement such as earthquake, land subsidence, and volcanic eruption. The paper by Sidiq et al. (2021) monitor land subsidence along the North Coast of Java Island in Indonesia from 2016 to 2020 by utilising C-Band Sentinel-1 and elevation data and Small Baseline Subset (SBAS) algorithm. Meanwhile, Tsuji et al. (2009) using L-band of ALOS PALSAR to compared the surface deformation before and after 2006 Yogyakarta earthquakes in Indonesia and Nuranjo et al. (2023) is the first paper to detected areas affected by volcanic ash deposits after an eruption in Mount Taal, Philippines using Temporal Decorrelation Model (TDM) applied for C-band of Sentinel-1. All of these papers Interferometric Synthetic Aperture Radar (InSAR) with different approaches. I think what makes SAR interesting in disaster mapping is that it allow us to do mapping in remote areas that hardly can be accessed if using terrestrial mapping or GNSS. Furthermore, the Sentinel-1 which is openly available allows us to do semi real-time mapping after the disaster happened so we can calculate a rough estimates of the affects and potential losses by it which make us more resilient to the disaster. However, it is unfortunate that Sentinel-1 only operates on C-band that has limitation in temporal decorrelation especially in vegetation areas compared to L-band as appointed by Sidiq et al. (2021) and Nuranjo et al. (2023). Tsuji et al. (2009) use L-band from ALOS PALSAR so it does not suffered the same problem with the other two studies, but ALOS PALSAR was discontinued in 2011 and ALOS PALSAR 2 data is limited to commercial. Sadly, the paper by Sidiq et al. (2021) and Tsuji et al. (200) do not provide accuracy assessment or compare the methods of InSAR with ground mapping. It would be great if we know how differ the land subsidence monitoring between the methods and how to improve the accuracy of InSAR to be the same level as ground monitoring. These two studies might adapting the approach by Nuranjo et al. (2023) that assessing their result with the field data from the officials. In the papers, they showed that they achieve more than 80% accuracy for their result, which is a good thing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "week8.html#reflection-upcoming-nasa-isro-nisar",
    "href": "week8.html#reflection-upcoming-nasa-isro-nisar",
    "title": "8  Introduction to Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflection: Upcoming NASA-ISRO NISAR",
    "text": "8.3 Reflection: Upcoming NASA-ISRO NISAR\nI think SAR is really interesting approaches to do time series mapping because it does not affected by weather and not rely on sun energy. It also can mapping remote areas in disaster mapping, so we can do almost real-time monitoring that does not requires lot of resources and allocation. However, understanding SAR and it’s data processing are extremely difficult and for now we can only use Sentinel-1 that is open dataset while we know that Sentinel-1 C-band is highly sensitive in vegetated areas and ALOS PALSAR 2 is limited to commercial. I tried to find other open SAR dataset that use L-band and find out NASA-ISRO NISAR that using L-band and S-band is expected to launch this year. I thin it will be massively used if the satellite has been launched later because the S-band provide the moderate performance between C-band and L-band so it can be more balanced and I am really excited on how these SAR dataset can be used for other mapping such as mapping ancient settlements, detection of buried archaeological structures, assessing building structure and stability, and carbon stock estimation. In January 2025, NASA-ISRO NISAR had finished all preliminary checkout and is expected to be launched in March 2025.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to Synthetic Aperture Radar (SAR)</span>"
    ]
  },
  {
    "objectID": "week7.html#summary-more-type-of-classification-and-accuracy-assessment",
    "href": "week7.html#summary-more-type-of-classification-and-accuracy-assessment",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "",
    "text": "Sub-pixel-based Classification\nSub-pixel classification recognizes that each pixel can contain multiple land-cover types. This method estimates the fractional composition of each pixel based on spectral properties. It’s especially useful when working with coarse-resolution imagery. Sub-pixel classification highly accurate for representing fractional information within mixed pixels. Particularly suitable for coarse resolution imagery where pixels represent multiple land covers. The accuracy significantly depends on correct endmember identification; incorrect or missing endmembers can cause misclassification and decrease overall accuracy.\nExamples of sub-pixel classification methods include:\n\nLinear Spectral Unmixing (LSU): Uses pure endmembers to estimate fractional abundances within each pixel.\nMultiple Endmember Spectral Mixture Analysis (MESMA): Allows different combinations of endmembers to model pixel composition dynamically.\n\n\n\nObject-based Classification\nObject-based classification groups pixels into meaningful objects or segments based on spatial, spectral, and textural properties before classification. Unlike traditional methods, object-based approaches classify objects rather than individual pixels, making use of spatial context. OBIA typically achieves higher classification accuracy compared to traditional pixel-based methods because it incorporates contextual information (shape, texture, spatial arrangement), reducing noise and improving class boundaries. However, accuracy strongly relies on segmentation quality; poor segmentation can lead to misclassification and reduce overall accuracy.\nExamples of object-based classification workflows typically involve two main stages:\n\nSegmentation: Pixels grouped into homogeneous objects based on spatial, spectral, and textural similarity.\nClassification: Objects classified into land cover categories using algorithms such as Random Forest, Support Vector Machine (SVM), or Decision Trees.\n\n\n\nClassification Accuracy Assessment\nAccuracy assessment is a process that should never be skipped when doing image classification. This process generally involves comparison between classified results and reference (ground-truth) data. The three most common types of accuracy measures are:\n\nProducer’s Accuracy: this measures how well a class in the reference data is correctly classified. It indicates omission error, reflecting cases where a pixel of a certain class was mistakenly omitted (missed).\nUser’s Accuracy: this measures the reliability from the user’s perspective, indicating commission errors. Commission errors happen when pixels are incorrectly classified into a category they do not belong to.\nOverall Accuracy: measures the overall performance by comparing the total number of correctly classified pixels with the total number of reference pixels.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  },
  {
    "objectID": "week5.html#application-global-artificial-impervious-area-gaia",
    "href": "week5.html#application-global-artificial-impervious-area-gaia",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "5.2 Application: Global Artificial Impervious Area (GAIA)",
    "text": "5.2 Application: Global Artificial Impervious Area (GAIA)\nGong, Li and Zhang (2019) map impervious surface in China from 1978 - 2017 using NDVI, MNDWI, and SWIR value of Landsat data that differentiate urban areas with other type of land cover. They also use Night Time Light from VIIRS for masking to reduce potential confusion between impervious cover and bare land, particularly in arid and semi-arid areas. By utilising exclusion/inclusion algorithm in GEE, they produce a map with 93% accuracy and 87% similarities with previous studies. I like this research because they use open remote sensing data and develop the algorithm through GEE which makes it reproducible for analysis in other areas and the result can be very useful for analysing urban expansion or differentiate the year the an area being built. Something that feels missing from this paper is that I do not find how the author dealing with different spatial resolution of Landsat 1, 2, 3 (60 meters) and Landsat 4, 5, 7, 8 (30 meters) and how it affect the performance of the analysis especially when they said that earlier data have higher accuracy and at the same time there is a limited good quality in earlier data which produce more uncertainty. Furthermore, the analysis performance does not doing well in arid or semi-arid, cold, and highly mountainous region implying nighttime light is not strong enough to help preserve settlements in those areas. But then, in 2020, the author published another similar paper with upgraded upgrade the analysis of impervious surface to global coverage and fully use 30 meter spatial resolution landsat images from 1985 – 2018 with Sentinel-1 complemented nighttime light data as ancillary datasets. In this upgrade, the backscatter coefficient of Sentinel-1 is used to generate a primary urban mask using a thresholding method for impervious surface mapping in arid region. The dataset that produce 89% of accuracy is being shared in GEE and can be used widely by users. I really like this dataset and use it a lot for my analysis. But, I also have some concern regarding how the low spatial resolution of Night Time Light can be used for delineating urban areas and how it might affect the overall spatial resolution of the result. Furthermore, I think it will be challenging to use the same method in current period where people start to design building using green roof because the methods rely on NDVI so it might be hard to differentiate building with vegetation areas.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week5.html#reflection-gee-which-provide-powerful-tools-and-reproducible-analysis",
    "href": "week5.html#reflection-gee-which-provide-powerful-tools-and-reproducible-analysis",
    "title": "5  Introduction to Google Earth Engine (GEE)",
    "section": "5.3 Reflection: GEE which Provide Powerful Tools and Reproducible Analysis",
    "text": "5.3 Reflection: GEE which Provide Powerful Tools and Reproducible Analysis\nI think GEE is the answer to limitation in computer resources when doing raster analysis. Because GEE is a cloud-based processing, we do not have to worry about the computer storage or drive storage space as we can save the result in assets. Furthermore, because of the cloud-based processing, we do not have to rely on high specification computer which helps me a lot in doing analysis. Although, good internet access is a requirement for GEE. Moreover, the massive already preprocessed dataset that available in GEE allows us to make more innovative analysis on uncommon topic. It will also saving more time because we could just use the product rather than raw raster that need to go through correction. Although, we still have to aware of the impact of the product resolution and accuracy to our analysis. Beside using raster product directly, we can also applied the algorithm made by other users to our analysis and even made changes if needed. The reproducible part of GEE makes development of research on remote sensing application even more rapid. The tools provided in GEE is also awesome and powerful where we can apply different machine learning algorithm for our analysis and display the result through interactive web map directly. Another thing that I like from GEE is that the result of data processing is automatically upsampling the pixel unlike GIS software like QGIS and ArcGIS that in default use downsampling. It makes the result automatically matched with the finest spatial resolution data that are used.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "week3.html#application-image-enhancement-and-atmospheric-correction-for-assessing-solar-pv-potential",
    "href": "week3.html#application-image-enhancement-and-atmospheric-correction-for-assessing-solar-pv-potential",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "3.2 Application: Image Enhancement and Atmospheric Correction for Assessing Solar PV Potential",
    "text": "3.2 Application: Image Enhancement and Atmospheric Correction for Assessing Solar PV Potential",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week6.html#application-machine-learning-techniques-in-habitat-suitability",
    "href": "week6.html#application-machine-learning-techniques-in-habitat-suitability",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "6.2 Application: Machine Learning Techniques in Habitat Suitability",
    "text": "6.2 Application: Machine Learning Techniques in Habitat Suitability\nPaper by Santoso et al. (2023) assess the suitability of the developed Grizzled leaf monkey habitat in Java Island, Indonesia using combination of Random Forest (RF), Support Vector Machine (SVM), and Maximum Entropy (MaxEnt). A total of 12 remote sensing product were used in this studies including MODIS LST, MODIS LC, CHIRPS, FLDAS, SRTM, HydroSHEDS, Landsat 8, VIIRS, Sentinel-5P and Worldpop. It is really impresive to me that nowadays image classification and machine learning methods are not only applied to land cover mapping, but for a variety use including habitat suitability. This paper give a comprehensive analysis by using massive dataset that assess the climate, topography, ecology, and anthropogenic factors to habitat suitability. They also integrating three different machine learning algorithms which I think can be good for eliminating limitation of each of the methods. However, I think it can be great if they use more technique in this integration rather than using unweighted one. I also like the layouting of the maps which is very creative and inspiring especially for Surili Habitat Against Population Prediction 2040. Things that I am curious about the method is thy why they use different vegetation index like NDVI, SAVI, and leaf area index because it potentially creating multicollinearity and affecting the result. I think it will be better if they can use the same procedure as Rahimian Boogar et al. (2019) in which they do proper data preprocessing and processing to predict habitat suitability of Juniperus spp. In the Southern Zagros Mountains of Iran by utilising SVM and MaxEnt. Both papers actually use massive dataset, but there are difference in parameters that they used in which Rahimian Boogar et al. (2019) divided the parameters into climate, topography, soil, and environmental factors. It will be interesting if Rahimian Boogar et al. (2019) also include anthropogenic factors in their study. Overall, I think what has been missing from both papers is that they only include AUC and ROC as accuracy assessment, it will be good if they include more accuracy assessment for more thorough analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week6.html#reflection-remote-sensing-for-thematic-mapping",
    "href": "week6.html#reflection-remote-sensing-for-thematic-mapping",
    "title": "6  Supervised vs Unsupervised Classification",
    "section": "6.3 Reflection: Remote Sensing for Thematic Mapping",
    "text": "6.3 Reflection: Remote Sensing for Thematic Mapping\nAs someone who really likes thematic mapping, I am extremely happy that remote sensing classification has been evolved to variety of topic such as habitat suitability. The method is not only glued to land cover analysis. Furthermore with the growth of machine learning, we do not have to manually create class based on AOI for doing unsupervised classification which also requires us interpret the analysis visually. But, unsupervised classification also provides a challenge because it depends on the availability of training dataset and the accuracy also depends on the quality of the dataset. Nevertheless, with the growth of dataset, I believe there will be more analysis that we can conduct using machine learning and remote sensing such as plastic waste detection in water bodies and informal settlement detection which I really interested.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Supervised vs Unsupervised Classification</span>"
    ]
  },
  {
    "objectID": "week3.html#application-correction-for-assessing-solar-pv-potential",
    "href": "week3.html#application-correction-for-assessing-solar-pv-potential",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "3.2 Application: Correction for Assessing Solar PV Potential",
    "text": "3.2 Application: Correction for Assessing Solar PV Potential\nThe paper by Sakti et al. (2022) model the potential of solar PV development of each building in Bandung, Indonesia by considering the energy potential and energy needed to generate electricity. The paper use shortwave radiation to calculate the solar energy potential in each building, but because sunlight that reach the building is not fully optimum, several correction methods should be applied by using Aerosol Optical Depth (AOD), precipitation, and Land Surface Temperature (LST). I think this paper is very interesting to support the government goals in transitioning to renewable energy. It provides a depth analysis on the solar energy potential and the electricity that needs to be generated. In the analysis on the solar energy potential, the author provides hillshade analysis and atmospheric correction that might alter the sunlight in reaching the rooftop. However, the author assumes constant value of daily data and does not consider cloud and humidity in the atmospheric correction which I think is important because beside needing optimum sunlight, we also have to make sure that the sun is shining almost all year round for it to be effective. Another paper that I found regarding this topic is by Lodhi et al. (2024) that assess the solar PV potential using Mask R-CNN and DeepLabV3 deep learning models on high resolution Google Earth Imagery and generating DEM from 10 meter resolution Sentinel-1 for estimating solar radiation. Both papers have same goals with very different approaches, but still applies image correction before extracting the value of solar raditation. Lodhi et al. (2024) uses noise removal, radio metric calibration, and terrain correction to Sentinel-1. I think for assessing solar PV potential, radiometric correction especially atmospheric correction is very important as it is connected directly to sun radiation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week3.html#reflection-the-overlook-importance-of-image-correction",
    "href": "week3.html#reflection-the-overlook-importance-of-image-correction",
    "title": "3  Remote Sensing Image Corrections and Enhancement",
    "section": "3.3 Reflection: The Overlook Importance of Image Correction",
    "text": "3.3 Reflection: The Overlook Importance of Image Correction\nAs I saw earlier in the application of atmospheric and radiometric correction in extracting solar radiation for assessing solar PV potential, image correction is really importance to generating real value. However, I find it these corrections are rarely talked about in the remote sensing application paper. But maybe because these days, a lot of remote sensing products have already been applied with image correction. Still, it is still importance to understand about the effect of atmosphere to the reflectance and how to use proper correction methods in the images. Beside for assessing solar PV potential, I think image correction and enhancement also important when doing study about Urban Heat Island (UHI), carbon sequestration, or mineral detection.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remote Sensing Image Corrections and Enhancement</span>"
    ]
  },
  {
    "objectID": "week7.html#reflection-the-complexities-of-sub-pixel-based-and-object-based-classification",
    "href": "week7.html#reflection-the-complexities-of-sub-pixel-based-and-object-based-classification",
    "title": "7  Sub-pixel-based vs Object-based Classification",
    "section": "7.3 Reflection: The Complexities of Sub-pixel-based and Object-based Classification",
    "text": "7.3 Reflection: The Complexities of Sub-pixel-based and Object-based Classification\nTruthfully, I think this topic is the most difficult one compared to other topics in this module. So far, building extraction is the only example that comes to mind when learning about object-based and sub-pixel-based classification. I find it quite tricky to distinguish between both, especially because these days, a lot of deep learning methods blur the line between pixel and object level analysis. What I realize is that object-based classification relies more on image segmentation, so the result heavily depends on how well the objects are grouped, while pixel-based methods classify every pixel without considering the spatial context, which makes it faster but also more prone to misclassification especially around boundaries. Other than for building extraction, I think methods can also be applied in various urban analyses such as detecting rooftop solar PV, greenhouses, green rooftops, or tree detection such as palm tree.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sub-pixel-based vs Object-based Classification</span>"
    ]
  }
]